# Fine-Tuning-M2M100-on-Nepali
Breaking the Language Barrier: Fine-Tuning M2M100 for English to Nepali Translation

# Fine-Tuning facebook/m2m100_418M for English-to-Nepali Translation

This project fine-tunes Meta's multilingual machine translation model [`facebook/m2m100_418M`] on an English-to-Nepali dataset to improve performance on low-resource translation tasks.

##  Objective

To adapt the pretrained `m2m100_418M` model specifically for **English ‚áÑ Nepali** translation using supervised fine-tuning and evaluate performance using BLEU, SacreBLEU, and BERTScore metrics.

---

## Dataset

- **Name:** [`CohleM/english-to-nepali`](https://huggingface.co/datasets/CohleM/english-to-nepali)
- **Source:** Hugging Face Datasets Hub
- **Languages:** English (`en`) ‚Üí Nepali (`ne`)
- **Format:** JSON format with `"translation"` field containing both `"en"` and `"ne"` keys

---

## Model

- Base model: [`facebook/m2m100_418M`](https://huggingface.co/facebook/m2m100_418M)
- Type: Sequence-to-sequence Transformer
- Tokenizer: `M2M100Tokenizer`

---

## Hyperparameters for Fine-Tuning

| Hyperparameter           | Value                  |
|--------------------------|------------------------|
| Model                    | facebook/m2m100_418M   |
| Batch size per device    | 8                      |
| Optimizer                | AdamW                  |
| Learning rate            | 2e-5                   |
| Epochs                   | 1                      |
| Total training steps     | `len(train_dataloader) * 1` |
| Learning rate scheduler  | Linear                 |
| Device                   | CUDA if available      |



## Example Translations

### Without Fine-Tuning

#### Easy
- **English:** My name is Gyawali.  
- **Nepali (Predicted):** ‡§Æ‡•á‡§∞‡•ã ‡§®‡§æ‡§Æ ‡§ú‡§ø‡§Ø‡§æ‡§µ‡§æ‡§≤‡•Ä ‡§π‡•ã ‡•§

#### Medium
- **English:** She likes to read books in the evening after finishing her homework.  
- **Nepali (Predicted):** ‡§ò‡§∞‡§¨‡•á‡§ü‡•Ä‡§≤‡•á ‡§ú‡§π‡§æ‡§Å ‡§™‡§æ‡§Ø‡•ã ‡§§‡•ç‡§Ø‡§π‡§æ‡§Å ‡§™‡§ï‡§æ‡§â‡§® ‡§¶‡§ø‡§Å‡§¶‡•à‡§® ‡•§

#### Hard
- **English:** Despite the economic challenges, the government is planning to invest more in renewable energy sources to ensure a sustainable future.  
- **Nepali (Predicted):** ‡§®‡§Ø‡§æ‡§Å ‡§∏‡§Ç‡§µ‡§ø‡§ß‡§æ‡§®‡§ï‡•ã ‡§ò‡•ã‡§∑‡§£‡§æ, ‡§Ü‡§®‡•ç‡§¶‡•ã‡§≤‡§®‡§ï‡•ã ‡§®‡§ø‡§∞‡§æ‡§∂‡§æ‡§ú‡§®‡§ï ‡§¨‡•à‡§†‡§æ‡§®, ‡§§‡•Ä‡§®‡•à ‡§§‡§π‡§ï‡•ã ‡§®‡§ø‡§∞‡•ç‡§µ‡§æ‡§ö‡§® ‡§™‡§∂‡•ç‡§ö‡§æ‡§§‡§ï‡•ã ‡§¶‡•á‡§∂ ‡§∞ ‡§Æ‡§ß‡•á‡§∏‡§ï‡•ã ‡§∏‡•ç‡§•‡§ø‡§§‡§ø‡§ï‡•ã ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ‡§ø‡§ï ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§π‡•Å‡§®‡•Å ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§õ ‡•§

---

## After Fine-Tuning

### --- Easy ---
- **English:** My name is Gyawali.  
- **Reference Nepali:** ‡§Æ‡•á‡§∞‡•ã ‡§®‡§æ‡§Æ ‡§ú‡•ç‡§û‡§µ‡§≤‡•Ä ‡§π‡•ã‡•§  
- **Predicted Nepali:** ‡§Æ‡•á‡§∞‡•ã ‡§®‡§æ‡§Æ ‡§ó‡§ø‡§µ‡§æ‡§≤‡•Ä ‡§π‡•ã‡•§


---

### --- Medium ---
- **English:** he likes to read books in the evening after finishing her homework.  
- **Reference Nepali:** ‡§â ‡§ó‡•É‡§π‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§∏‡§ï‡•á‡§™‡§õ‡§ø ‡§∏‡§æ‡§Å‡§ù ‡§ï‡§ø‡§§‡§æ‡§¨ ‡§™‡§¢‡•ç‡§® ‡§Æ‡§® ‡§™‡§∞‡§æ‡§â‡§Å‡§õ‡•§  
- **Predicted Nepali:** ‡§â‡§®‡•Ä ‡§Ü‡§´‡•ç‡§®‡•ã ‡§ó‡•É‡§π ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§∏‡§Æ‡§æ‡§™‡•ç‡§§ ‡§≠‡§è‡§™‡§õ‡§ø ‡§∏‡§æ‡§Å‡§ù‡§Æ‡§æ ‡§ï‡§ø‡§§‡§æ‡§¨ ‡§™‡§¢‡•ç‡§® ‡§Æ‡§® ‡§™‡§∞‡§æ‡§â‡§Å‡§õ‡§®‡•ç‡•§


---

### --- Hard ---
- **English:** Despite the economic challenges, the government is planning to invest more in renewable energy sources to ensure a sustainable future.  
- **Reference Nepali:** ‡§Ü‡§∞‡•ç‡§•‡§ø‡§ï ‡§ö‡•Å‡§®‡•å‡§§‡•Ä‡§π‡§∞‡•Ç‡§ï‡§æ ‡§¨‡§æ‡§¨‡§ú‡•Å‡§¶, ‡§∏‡§∞‡§ï‡§æ‡§∞‡§≤‡•á ‡§¶‡§ø‡§ó‡•ã ‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø ‡§∏‡•Å‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§ó‡§∞‡•ç‡§® ‡§®‡§µ‡•Ä‡§ï‡§∞‡§£‡•Ä‡§Ø ‡§ä‡§∞‡•ç‡§ú‡§æ ‡§∏‡•ç‡§∞‡•ã‡§§‡§π‡§∞‡•Ç‡§Æ‡§æ ‡§•‡§™ ‡§≤‡§ó‡§æ‡§®‡•Ä ‡§ó‡§∞‡•ç‡§®‡•á ‡§Ø‡•ã‡§ú‡§®‡§æ ‡§¨‡§®‡§æ‡§á‡§∞‡§π‡•á‡§ï‡•ã ‡§õ‡•§  
- **Predicted Nepali:** ‡§Ü‡§∞‡•ç‡§•‡§ø‡§ï ‡§ö‡•Å‡§®‡•å‡§§‡§ø‡§ï‡§æ ‡§¨‡§æ‡§¨‡§ú‡•Å‡§¶ ‡§∏‡§∞‡§ï‡§æ‡§∞‡§≤‡•á ‡§¶‡§ø‡§ó‡•ã ‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø‡§≤‡§æ‡§à ‡§∏‡•Å‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§ó‡§∞‡•ç‡§® ‡§®‡§µ‡•Ä‡§ï‡§∞‡§£‡•Ä‡§Ø ‡§ä‡§∞‡•ç‡§ú‡§æ‡§ï‡•ã ‡§∏‡•ç‡§∞‡•ã‡§§‡§Æ‡§æ ‡§¨‡§¢‡•Ä ‡§≤‡§ó‡§æ‡§®‡•Ä ‡§ó‡§∞‡•ç‡§®‡•á ‡§Ø‡•ã‡§ú‡§®‡§æ ‡§∞‡§π‡•á‡§ï‡•ã ‡§õ‡•§


---
## Evaluation Metrics

| Difficulty | BLEU (NLTK) | SacreBLEU | BERTScore F1 | Notes |
|------------|-------------|-----------|---------------|-------|
| Easy       | 0.1862      | 35.36     | 0.8944        | Minor spelling error |
| Medium     | 0.1161      | 13.95     | 0.8889        | Formal paraphrasing |
| Hard       | 0.1441      | 16.61     | 0.9069        | Great semantic accuracy |

- **BLEU**: Penalizes rephrasing and grammar differences
- **SacreBLEU**: Better standardized BLEU variant
- **BERTScore**: High semantic similarity indicates good translation quality

---

## üèÅ How to Run

### 1. Clone this repo & install dependencies
```bash
git clone https://github.com/yourusername/english-nepali-m2m100.git
cd english-nepali-m2m100
pip install -r requirements.txt

## Conclusion

- The base model (`facebook/m2m100_418M`) struggled with accurate translation before fine-tuning, especially in medium and hard cases.
- After fine-tuning:
  - **Semantic accuracy** (BERTScore) improved significantly across all difficulty levels.
  - **Lexical accuracy** (BLEU, SacreBLEU) improved, though remained relatively low due to paraphrasing and stylistic changes.
- This suggests the model is **learning meaningful translation patterns**, even if exact phrasing differs.

